{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13dca5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mounted at /content/drive\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "# User-Agent\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2095de39",
   "metadata": {},
   "source": [
    "### Overview\n",
    "This code sets up the necessary tools and headers to scrape web data. It imports libraries for sending HTTP requests, parsing HTML, and handling data. Additionally, it defines a user agent header to mimic a web browser, which can be useful to avoid getting blocked by some websites.\n",
    "\n",
    "***Import requests**\n",
    "    This line imports the requests module, which is a popular Python module used to send HTTP requests to websites.\n",
    "* **From bs4 import BeautifulSoup**\n",
    "    This line imports the BeautifulSoup from the bs4 module. BeautifulSoup is a library that is used for web scraping purposes to pull the data out of HTML and XML files. It creates a parse tree that can be used to extract data in a hierarchical and more readable manner.\n",
    "* **Import pandas as pd**\n",
    "    This line imports the pandas library, which is a powerful data manipulation and analysis tool. You will use it to structure and analyze the data you scrape from the websites.\n",
    "* **Import os**\n",
    "    This line imports the os module, which is used for interacting with the operating system. This could be used for tasks like creating directories, reading environment variables, etc.\n",
    "* **Import time**\n",
    "    This line imports the time module, which can be used to add delays to your code. Adding delays is a good practice when web scraping to avoid sending too many requests too quickly, which could lead to you getting blocked by the website.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072233c9",
   "metadata": {},
   "source": [
    "# Extracting Flats/Apartments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a151c24",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    715\u001b[0m     conn,\n\u001b[0;32m    716\u001b[0m     method,\n\u001b[0;32m    717\u001b[0m     url,\n\u001b[0;32m    718\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    719\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    720\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    721\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    722\u001b[0m )\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m             six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 461\u001b[0m     httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m     response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:798\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    796\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m--> 798\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    799\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39me, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    801\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:550\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 550\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m six\u001b[38;5;241m.\u001b[39mreraise(\u001b[38;5;28mtype\u001b[39m(error), error, _stacktrace)\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\packages\\six.py:769\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m    770\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    715\u001b[0m     conn,\n\u001b[0;32m    716\u001b[0m     method,\n\u001b[0;32m    717\u001b[0m     url,\n\u001b[0;32m    718\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    719\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    720\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    721\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    722\u001b[0m )\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m             six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 461\u001b[0m     httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m     response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m pageNumber \u001b[38;5;241m<\u001b[39m end:\n\u001b[0;32m     39\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.99acres.com/flats-in-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-ffid-page-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpageNumber\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 40\u001b[0m     page \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m     42\u001b[0m     pageSoup \u001b[38;5;241m=\u001b[39m BeautifulSoup(page\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Rest of the code for parsing and extracting data would go here...\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:501\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))"
     ]
    }
   ],
   "source": [
    "# mounted at /content/drive\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "# User-Agent\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'}\n",
    "\n",
    "flats = pd.DataFrame()\n",
    "\n",
    "start = 127\n",
    "end = 200\n",
    "csv_file = f\"/content/drive/MyDrive/DSP/Case Studies/Real estate/Flats_gurgaon_data_p{start}-{end}.csv\"\n",
    "\n",
    "pageNumber = start\n",
    "req=0\n",
    "city = 'gurgaon'\n",
    "\n",
    "while pageNumber < end:\n",
    "    url = f\"https://www.99acres.com/flats-in-{city}-ffid-page-{pageNumber}\"\n",
    "    page = requests.get(url, headers=headers)\n",
    "    \n",
    "    pageSoup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # Rest of the code for parsing and extracting data would go here...\n",
    "    for soup in pageSoup.select_one('div[data-label=\"SEARCH\"]').select('section[data-hydration-on-demand=\"true\"]'):\n",
    "        # Extract property name and property sub-name\n",
    "        try:\n",
    "            property_name = soup.select_one('a.srpTuple_propertyName').text.strip()\n",
    "            link = soup.select_one('a.srpTuple_propertyName')['href']\n",
    "            society = soup.select_one('div#srp_tuple_society_heading').text.strip()\n",
    "        except:\n",
    "            property_name = None\n",
    "            link = None\n",
    "            society = None\n",
    "            continue # If there's an error on the main page, skip to the next item\n",
    "\n",
    "        # Detail Page\n",
    "        page = requests.get(link, headers=headers)\n",
    "        dpageSoup = BeautifulSoup(page.content, 'html.parser')\n",
    "        req += 1\n",
    "\n",
    "        try:\n",
    "            # Price Range\n",
    "            price = dpageSoup.select_one('#pdPrice2').text.strip()\n",
    "        except:\n",
    "            price = None\n",
    "            \n",
    "        try:\n",
    "            # Area\n",
    "            area = dpageSoup.select_one('#srp_tuple_price_per_unit_area').text.strip()\n",
    "        except:\n",
    "            area = None\n",
    "            \n",
    "        try:\n",
    "            # Area with Type\n",
    "            areawithType = dpageSoup.select_one('#FartArea').text.strip()\n",
    "        except:\n",
    "            areawithType = None\n",
    "            \n",
    "        try:\n",
    "            # Bedroom\n",
    "            bedroom = dpageSoup.select_one('div.srpTuple__bedroomNum').text.strip()\n",
    "        except:\n",
    "            bedroom = None\n",
    "            \n",
    "        try:\n",
    "            # Bathroom\n",
    "            bathroom = dpageSoup.select_one('div.srpTuple__bathroomNum').text.strip()\n",
    "        except:\n",
    "            bathroom = None\n",
    "        try:\n",
    "            # Balcony\n",
    "            balcony = dpageSoup.select_one('div.srpTuple__balconyNum').text.strip()\n",
    "        except:\n",
    "            balcony = None    \n",
    "        try:\n",
    "            # Parking\n",
    "            parking = dpageSoup.select_one('div.srpTuple__parking').text.strip()\n",
    "        except:\n",
    "            parking = None\n",
    "\n",
    "        try:\n",
    "            # Address\n",
    "            address = dpageSoup.select_one('div#address').text.strip()\n",
    "        except:\n",
    "            address = None\n",
    "\n",
    "        try:\n",
    "            # Floor Number\n",
    "            floorNum = dpageSoup.select_one('div#floor_number').text.strip()\n",
    "        except:\n",
    "            floorNum = None\n",
    "        \n",
    "        try:\n",
    "            # Facing\n",
    "            facing = dpageSoup.select_one('div#facing').text.strip()\n",
    "        except:\n",
    "            facing = None\n",
    "\n",
    "        try:\n",
    "            # Age of Possession\n",
    "            agePossession = dpageSoup.select_one('div#agePossessionLbl').text.strip()\n",
    "        except:\n",
    "            agePossession = None\n",
    "        \n",
    "        try:\n",
    "            # Nearby Locations\n",
    "            nearbyLocations = [i.text.strip() for i in dpageSoup.select_one('div.nearbyLocation_tagWrap').select('span.nearbyLocation_infoText')]\n",
    "        except:\n",
    "            nearbyLocations = []\n",
    "\n",
    "        try:\n",
    "            # Description\n",
    "            description = dpageSoup.select_one('div.srp_tuple_description').text.strip()\n",
    "        except:\n",
    "            description = None\n",
    "\n",
    "        try:\n",
    "            # Furnish Details\n",
    "            furnishDetails = [i.text.strip() for i in dpageSoup.select_one('ul#FurnishDetails').select('li')]\n",
    "        except:\n",
    "            furnishDetails = []\n",
    "\n",
    "        # Features\n",
    "        if furnishDetails:\n",
    "            try:\n",
    "                features = [i.text.strip() for i in dpageSoup.select_one('ul#features').select('li')]\n",
    "            except:\n",
    "                features = []\n",
    "        else:\n",
    "            try:\n",
    "                features = [i.text.strip() for i in dpageSoup.select_one('ul#features').select('li')]\n",
    "            except:\n",
    "                features = []\n",
    "                \n",
    "        try:\n",
    "            # Rating\n",
    "            rating = [i.text.strip() for i in dpageSoup.select_one('div.review_rightSide_div_1_5').select('div.ratingByFeature_circleWrap')]\n",
    "        except:\n",
    "            rating = None\n",
    "            \n",
    "        try:\n",
    "            # Property ID\n",
    "            property_id = dpageSoup.select_one('div#prop_id').text.strip()\n",
    "        except:\n",
    "            property_id = None\n",
    "            \n",
    "        # create a dictionary with the given variables\n",
    "        property_data = {\n",
    "            'property_name': property_name,\n",
    "            'link': link,\n",
    "            'society': society,\n",
    "            'price': price,\n",
    "            'area': area,\n",
    "            'areawithType': areawithType,\n",
    "            'bedroom': bedroom,\n",
    "            'bathroom': bathroom,\n",
    "            'balcony': balcony,\n",
    "            'additionalRooms': None, # This was not in your screenshots but is a good practice\n",
    "            'address': address,\n",
    "            'floorNum': floorNum,\n",
    "            'facing': facing,\n",
    "            'agePossession': agePossession,\n",
    "            'nearbyLocations': nearbyLocations,\n",
    "            'description': description,\n",
    "            'furnishDetails': furnishDetails,\n",
    "            'features': features,\n",
    "            'rating': rating,\n",
    "            'property_id': property_id\n",
    "        }\n",
    "        \n",
    "    temp_df = pd.DataFrame.from_records([property_data])\n",
    "    # print(temp_df)\n",
    "    flats = pd.concat([flats, temp_df], ignore_index=True)\n",
    "    \n",
    "    if os.path.isfile(csv_file):\n",
    "        # Append DataFrame to the existing file without header\n",
    "        temp_df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # Write DataFrame to the file with header\n",
    "        temp_df.to_csv(csv_file, mode='a', header=True, index=False)\n",
    "        \n",
    "    if req % 4 == 0:\n",
    "        time.sleep(10)\n",
    "    if req % 15 == 0:\n",
    "        time.sleep(50)\n",
    "    \n",
    "    print(f\"{pageNumber} -> {req}\")\n",
    "    pageNumber += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b3403a",
   "metadata": {},
   "source": [
    "The code scrapes property data from the website \"99acres.com\" for apartments in Gurgaon. It navigates through a range of pages, extracts details of each property, and saves the data to a CSV file. The script is designed to handle potential errors gracefully, using try and except blocks to manage missing data, and introduces pauses to avoid making rapid requests and potentially getting blocked by the website\n",
    "Initialization of Variables:\n",
    "⚫ start and end specify the range of web pages to scrape.\n",
    "csv file defines the path to the CSV file where data will be saved.\n",
    "pageNumber starts from the initial value of start and will be incremented to navigate through the pages.\n",
    "req counts the number of HTTP requests made.\n",
    "Loop for Page Navigation\n",
    "The while loop is used to navigate through each page in the range from start to end.\n",
    "Inside this loop, the URL of the page to be scraped is constructed using the pageNumber.\n",
    "An HTTP GET request is made to retrieve the content of the page, and the content is then parsed using BeautifulSoup.\n",
    "Loop for Property Extraction.\n",
    "The nested for loop navigates through individual property sections on the current page.\n",
    "The script attempts to extract the property name, its link, and its society name.\n",
    "If any of these attributes are missing, it skips to the next property.\n",
    "00:33:44\n",
    "For each property, an HTTP request is made to its detail page\n",
    "1.\n",
    "Session 1 on Capstone Project - Data Gathering property details sxe pike, aida. Leam Sabunt, battitom count, balcony count address, and many other attributes. if any attribute is missing the code handles it gracefully, assigning an empty string or an empty list as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb930c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New: Create a session with a retry strategy\n",
    "session = requests.Session()\n",
    "retry_strategy = Retry(\n",
    "    total=3,  # Number of retries\n",
    "    backoff_factor=1,  # Delay factor (1s, 2s, 4s...)\n",
    "    status_forcelist=[429, 500, 502, 503, 504]\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "\n",
    "# Your existing code\n",
    "flats = pd.DataFrame()\n",
    "\n",
    "start = 127\n",
    "end = 200\n",
    "csv_file = f\"/content/drive/MyDrive/DSP/Case Studies/Real estate/Flats_gurgaon_data_p{start}-{end}.csv\"\n",
    "\n",
    "pageNumber = start\n",
    "req=0\n",
    "city = 'gurgaon'\n",
    "\n",
    "while pageNumber < end:\n",
    "    url = f\"https://www.99acres.com/flats-in-{city}-ffid-page-{pageNumber}\"\n",
    "    \n",
    "    # Use the session to make requests\n",
    "    try:\n",
    "        page = session.get(url, headers=headers)\n",
    "        pageSoup = BeautifulSoup(page.content, 'html.parser')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making request: {e}\")\n",
    "        pageNumber += 1\n",
    "        continue\n",
    "    \n",
    "    search_results = pageSoup.select_one('div[data-label=\"SEARCH\"]')\n",
    "    \n",
    "    if search_results:\n",
    "        for soup in search_results.select('section[data-hydration-on-demand=\"true\"]'):\n",
    "            # Extract property name and property sub-name\n",
    "            try:\n",
    "                property_name = soup.select_one('a.srpTuple_propertyName').text.strip()\n",
    "                link = soup.select_one('a.srpTuple_propertyName')['href']\n",
    "                society = soup.select_one('div#srp_tuple_society_heading').text.strip()\n",
    "            except:\n",
    "                property_name = None\n",
    "                link = None\n",
    "                society = None\n",
    "                continue # If there's an error on the main page, skip to the next item\n",
    "\n",
    "            # Detail Page\n",
    "            try:\n",
    "                page = session.get(link, headers=headers)\n",
    "                dpageSoup = BeautifulSoup(page.content, 'html.parser')\n",
    "                req += 1\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error making request to detail page: {e}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Price Range\n",
    "                price = dpageSoup.select_one('#pdPrice2').text.strip()\n",
    "            except:\n",
    "                price = None\n",
    "                \n",
    "            try:\n",
    "                # Area\n",
    "                area = dpageSoup.select_one('#srp_tuple_price_per_unit_area').text.strip()\n",
    "            except:\n",
    "                area = None\n",
    "                \n",
    "            try:\n",
    "                # Area with Type\n",
    "                areawithType = dpageSoup.select_one('#FartArea').text.strip()\n",
    "            except:\n",
    "                areawithType = None\n",
    "                \n",
    "            try:\n",
    "                # Bedroom\n",
    "                bedroom = dpageSoup.select_one('div.srpTuple__bedroomNum').text.strip()\n",
    "            except:\n",
    "                bedroom = None\n",
    "                \n",
    "            try:\n",
    "                # Bathroom\n",
    "                bathroom = dpageSoup.select_one('div.srpTuple__bathroomNum').text.strip()\n",
    "            except:\n",
    "                bathroom = None\n",
    "            try:\n",
    "                # Balcony\n",
    "                balcony = dpageSoup.select_one('div.srpTuple__balconyNum').text.strip()\n",
    "            except:\n",
    "                balcony = None    \n",
    "            try:\n",
    "                # Parking\n",
    "                parking = dpageSoup.select_one('div.srpTuple__parking').text.strip()\n",
    "            except:\n",
    "                parking = None\n",
    "\n",
    "            try:\n",
    "                # Address\n",
    "                address = dpageSoup.select_one('div#address').text.strip()\n",
    "            except:\n",
    "                address = None\n",
    "\n",
    "            try:\n",
    "                # Floor Number\n",
    "                floorNum = dpageSoup.select_one('div#floor_number').text.strip()\n",
    "            except:\n",
    "                floorNum = None\n",
    "            \n",
    "            try:\n",
    "                # Facing\n",
    "                facing = dpageSoup.select_one('div#facing').text.strip()\n",
    "            except:\n",
    "                facing = None\n",
    "\n",
    "            try:\n",
    "                # Age of Possession\n",
    "                agePossession = dpageSoup.select_one('div#agePossessionLbl').text.strip()\n",
    "            except:\n",
    "                agePossession = None\n",
    "            \n",
    "            try:\n",
    "                # Nearby Locations\n",
    "                nearbyLocations = [i.text.strip() for i in dpageSoup.select_one('div.nearbyLocation_tagWrap').select('span.nearbyLocation_infoText')]\n",
    "            except:\n",
    "                nearbyLocations = []\n",
    "\n",
    "            try:\n",
    "                # Description\n",
    "                description = dpageSoup.select_one('div.srp_tuple_description').text.strip()\n",
    "            except:\n",
    "                description = None\n",
    "\n",
    "            try:\n",
    "                # Furnish Details\n",
    "                furnishDetails = [i.text.strip() for i in dpageSoup.select_one('ul#FurnishDetails').select('li')]\n",
    "            except:\n",
    "                furnishDetails = []\n",
    "\n",
    "            # Features\n",
    "            if furnishDetails:\n",
    "                try:\n",
    "                    features = [i.text.strip() for i in dpageSoup.select_one('ul#features').select('li')]\n",
    "                except:\n",
    "                    features = []\n",
    "            else:\n",
    "                try:\n",
    "                    features = [i.text.strip() for i in dpageSoup.select_one('ul#features').select('li')]\n",
    "                except:\n",
    "                    features = []\n",
    "                    \n",
    "            try:\n",
    "                # Rating\n",
    "                rating = [i.text.strip() for i in dpageSoup.select_one('div.review_rightSide_div_1_5').select('div.ratingByFeature_circleWrap')]\n",
    "            except:\n",
    "                rating = None\n",
    "                \n",
    "            try:\n",
    "                # Property ID\n",
    "                property_id = dpageSoup.select_one('div#prop_id').text.strip()\n",
    "            except:\n",
    "                property_id = None\n",
    "                \n",
    "            # create a dictionary with the given variables\n",
    "            property_data = {\n",
    "                'property_name': property_name,\n",
    "                'link': link,\n",
    "                'society': society,\n",
    "                'price': price,\n",
    "                'area': area,\n",
    "                'areawithType': areawithType,\n",
    "                'bedroom': bedroom,\n",
    "                'bathroom': bathroom,\n",
    "                'balcony': balcony,\n",
    "                'additionalRooms': None, # This was not in your screenshots but is a good practice\n",
    "                'address': address,\n",
    "                'floorNum': floorNum,\n",
    "                'facing': facing,\n",
    "                'agePossession': agePossession,\n",
    "                'nearbyLocations': nearbyLocations,\n",
    "                'description': description,\n",
    "                'furnishDetails': furnishDetails,\n",
    "                'features': features,\n",
    "                'rating': rating,\n",
    "                'property_id': property_id\n",
    "            }\n",
    "            \n",
    "        temp_df = pd.DataFrame.from_records([property_data])\n",
    "        # print(temp_df)\n",
    "        flats = pd.concat([flats, temp_df], ignore_index=True)\n",
    "        \n",
    "        if os.path.isfile(csv_file):\n",
    "            # Append DataFrame to the existing file without header\n",
    "            temp_df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            # Write DataFrame to the file with header\n",
    "            temp_df.to_csv(csv_file, mode='a', header=True, index=False)\n",
    "            \n",
    "        if req % 5 == 0:\n",
    "            time.sleep(10)\n",
    "        if req % 15 == 0:\n",
    "            time.sleep(50)\n",
    "        \n",
    "        print(f\"{pageNumber} -> {req}\")\n",
    "    pageNumber += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef414930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mounted at /content/drive\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "# User-Agent\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'}\n",
    "\n",
    "# New: Create a session with a retry strategy\n",
    "session = requests.Session()\n",
    "retry_strategy = Retry(\n",
    "    total=3,  # Number of retries\n",
    "    backoff_factor=1,  # Delay factor (1s, 2s, 4s...)\n",
    "    status_forcelist=[429, 500, 502, 503, 504]\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "\n",
    "# Your existing code\n",
    "flats = pd.DataFrame()\n",
    "\n",
    "start = 1\n",
    "end = 10\n",
    "csv_file = f\"/content/drive/MyDrive/DSP/Case Studies/Real estate/Flats_gurgaon_housing_data_p{start}-{end}.csv\"\n",
    "\n",
    "pageNumber = start\n",
    "req = 0\n",
    "city = 'gurgaon'\n",
    "\n",
    "while pageNumber < end:\n",
    "    url = f\"https://housing.com/in/buy/flats/p/{city}?page={pageNumber}\"\n",
    "    \n",
    "    # Use the session to make requests\n",
    "    try:\n",
    "        page = session.get(url, headers=headers)\n",
    "        pageSoup = BeautifulSoup(page.content, 'html.parser')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making request: {e}\")\n",
    "        pageNumber += 1\n",
    "        continue\n",
    "    \n",
    "    # The new search result selector for Housing.com\n",
    "    search_results = pageSoup.find_all('div', {'data-testid': 'listing-card'})\n",
    "    \n",
    "    if search_results:\n",
    "        for soup in search_results:\n",
    "            # Extract property name and property sub-name\n",
    "            try:\n",
    "                property_name = soup.find('h2', {'data-testid': 'listing-card-title'}).text.strip()\n",
    "                link = soup.find('a', {'data-testid': 'listing-card-link'})['href']\n",
    "                society = soup.find('div', {'data-testid': 'listing-card-title-address'}).text.strip()\n",
    "            except:\n",
    "                property_name = None\n",
    "                link = None\n",
    "                society = None\n",
    "                continue # If there's an error on the main page, skip to the next item\n",
    "\n",
    "            # Detail Page\n",
    "            try:\n",
    "                page = session.get(link, headers=headers)\n",
    "                dpageSoup = BeautifulSoup(page.content, 'html.parser')\n",
    "                req += 1\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error making request to detail page: {e}\")\n",
    "                continue\n",
    "\n",
    "            # This part will need to be updated with new selectors for the detail page of Housing.com\n",
    "            # As the structure is different, you will need to inspect the elements of a Housing.com detail page\n",
    "            # to find the correct selectors for price, area, bedroom, etc.\n",
    "            \n",
    "            # For now, we will add placeholders. You can fill these in after inspecting the page.\n",
    "            price = None\n",
    "            area = None\n",
    "            areawithType = None\n",
    "            bedroom = None\n",
    "            bathroom = None\n",
    "            balcony = None\n",
    "            parking = None\n",
    "            address = None\n",
    "            floorNum = None\n",
    "            facing = None\n",
    "            agePossession = None\n",
    "            nearbyLocations = []\n",
    "            description = None\n",
    "            furnishDetails = []\n",
    "            features = []\n",
    "            rating = None\n",
    "            property_id = None\n",
    "                \n",
    "            # create a dictionary with the given variables\n",
    "            property_data = {\n",
    "                'property_name': property_name,\n",
    "                'link': link,\n",
    "                'society': society,\n",
    "                'price': price,\n",
    "                'area': area,\n",
    "                'areawithType': areawithType,\n",
    "                'bedroom': bedroom,\n",
    "                'bathroom': bathroom,\n",
    "                'balcony': balcony,\n",
    "                'additionalRooms': None, \n",
    "                'address': address,\n",
    "                'floorNum': floorNum,\n",
    "                'facing': facing,\n",
    "                'agePossession': agePossession,\n",
    "                'nearbyLocations': nearbyLocations,\n",
    "                'description': description,\n",
    "                'furnishDetails': furnishDetails,\n",
    "                'features': features,\n",
    "                'rating': rating,\n",
    "                'property_id': property_id\n",
    "            }\n",
    "            \n",
    "        temp_df = pd.DataFrame.from_records([property_data])\n",
    "        flats = pd.concat([flats, temp_df], ignore_index=True)\n",
    "        \n",
    "        if os.path.isfile(csv_file):\n",
    "            temp_df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            temp_df.to_csv(csv_file, mode='a', header=True, index=False)\n",
    "            \n",
    "        if req % 5 == 0:\n",
    "            time.sleep(10)\n",
    "        if req % 15 == 0:\n",
    "            time.sleep(50)\n",
    "        \n",
    "        print(f\"{pageNumber} -> {req}\")\n",
    "    pageNumber += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20ce76b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mounted at /content/drive\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "# User-Agent\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'}\n",
    "\n",
    "# New: Create a session with a retry strategy\n",
    "session = requests.Session()\n",
    "retry_strategy = Retry(\n",
    "    total=3,  # Number of retries\n",
    "    backoff_factor=1,  # Delay factor (1s, 2s, 4s...)\n",
    "    status_forcelist=[429, 500, 502, 503, 504]\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "\n",
    "# Your existing code\n",
    "flats = pd.DataFrame()\n",
    "\n",
    "start = 1\n",
    "end = 10\n",
    "csv_file = f\"/content/drive/MyDrive/DSP/Case Studies/Real estate/Flats_gurgaon_housing_data_p{start}-{end}.csv\"\n",
    "\n",
    "pageNumber = start\n",
    "req = 0\n",
    "city = 'gurgaon'\n",
    "\n",
    "while pageNumber < end:\n",
    "    url = f\"https://housing.com/in/buy/flats/p/{city}?page={pageNumber}\"\n",
    "    \n",
    "    # Use the session to make requests\n",
    "    try:\n",
    "        page = session.get(url, headers=headers)\n",
    "        pageSoup = BeautifulSoup(page.content, 'html.parser')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making request: {e}\")\n",
    "        pageNumber += 1\n",
    "        continue\n",
    "    \n",
    "    # The new search result selector for Housing.com\n",
    "    search_results = pageSoup.find_all('div', {'data-testid': 'listing-card'})\n",
    "    \n",
    "    if search_results:\n",
    "        for soup in search_results:\n",
    "            # Extract property name and property sub-name\n",
    "            try:\n",
    "                property_name = soup.find('h2', {'data-testid': 'listing-card-title'}).text.strip()\n",
    "                link = soup.find('a', {'data-testid': 'listing-card-link'})['href']\n",
    "                society = soup.find('div', {'data-testid': 'listing-card-title-address'}).text.strip()\n",
    "            except:\n",
    "                property_name = None\n",
    "                link = None\n",
    "                society = None\n",
    "                continue # If there's an error on the main page, skip to the next item\n",
    "\n",
    "            # Detail Page\n",
    "            try:\n",
    "                # Add a longer, randomized sleep here to appear more human-like\n",
    "                time.sleep(1 + time.random() * 2) \n",
    "                page = session.get(link, headers=headers)\n",
    "                dpageSoup = BeautifulSoup(page.content, 'html.parser')\n",
    "                req += 1\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error making request to detail page: {e}\")\n",
    "                continue\n",
    "\n",
    "            # This part will need to be updated with new selectors for the detail page of Housing.com\n",
    "            # As the structure is different, you will need to inspect the elements of a Housing.com detail page\n",
    "            # to find the correct selectors for price, area, bedroom, etc.\n",
    "            \n",
    "            # For now, we will add placeholders. You can fill these in after inspecting the page.\n",
    "            price = None\n",
    "            area = None\n",
    "            areawithType = None\n",
    "            bedroom = None\n",
    "            bathroom = None\n",
    "            balcony = None\n",
    "            parking = None\n",
    "            address = None\n",
    "            floorNum = None\n",
    "            facing = None\n",
    "            agePossession = None\n",
    "            nearbyLocations = []\n",
    "            description = None\n",
    "            furnishDetails = []\n",
    "            features = []\n",
    "            rating = None\n",
    "            property_id = None\n",
    "                \n",
    "            # create a dictionary with the given variables\n",
    "            property_data = {\n",
    "                'property_name': property_name,\n",
    "                'link': link,\n",
    "                'society': society,\n",
    "                'price': price,\n",
    "                'area': area,\n",
    "                'areawithType': areawithType,\n",
    "                'bedroom': bedroom,\n",
    "                'bathroom': bathroom,\n",
    "                'balcony': balcony,\n",
    "                'additionalRooms': None, \n",
    "                'address': address,\n",
    "                'floorNum': floorNum,\n",
    "                'facing': facing,\n",
    "                'agePossession': agePossession,\n",
    "                'nearbyLocations': nearbyLocations,\n",
    "                'description': description,\n",
    "                'furnishDetails': furnishDetails,\n",
    "                'features': features,\n",
    "                'rating': rating,\n",
    "                'property_id': property_id\n",
    "            }\n",
    "            \n",
    "        temp_df = pd.DataFrame.from_records([property_data])\n",
    "        flats = pd.concat([flats, temp_df], ignore_index=True)\n",
    "        \n",
    "        if os.path.isfile(csv_file):\n",
    "            temp_df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            temp_df.to_csv(csv_file, mode='a', header=True, index=False)\n",
    "        \n",
    "        # Increased sleep time for pages to avoid throttling\n",
    "        print(f\"Pausing for 30 seconds to avoid being blocked...\")\n",
    "        time.sleep(30)\n",
    "        \n",
    "        print(f\"{pageNumber} -> {req}\")\n",
    "    pageNumber += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "922f77cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 0 entries\n",
      "Empty DataFrame\n"
     ]
    }
   ],
   "source": [
    "flats.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c12ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
